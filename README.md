# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-1.8%2B-red)
![Task](https://img.shields.io/badge/Task-Paper__Review_%26_Implementation-green)

## ğŸ“Œ Project Overview
ì´ ì €ì¥ì†ŒëŠ” **BERT (Devlin et al., 2018)** ë…¼ë¬¸ì„ ê¹Šì´ ìˆê²Œ ë¶„ì„í•˜ê³ , ì‹¤ì œ ì½”ë“œ êµ¬í˜„ì„ í†µí•´ ê·¸ ì„±ëŠ¥ì„ ê²€ì¦í•œ ìŠ¤í„°ë”” ê¸°ë¡ì…ë‹ˆë‹¤.

í¬ê²Œ ë‘ ê°€ì§€ íŒŒíŠ¸ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤:
1.  **Paper Review:** ë…¼ë¬¸ì˜ í•µì‹¬ ì•„ì´ë””ì–´(MLM, NSP, Architecture) ì •ë¦¬ ë° ë°œí‘œ ìë£Œ ì œì‘.
2.  **Implementation:** IMDb ê°ì„± ë¶„ì„ íƒœìŠ¤í¬ë¥¼ í†µí•´ BERTì™€ GPTì˜ ì„±ëŠ¥ì„ ì§ì ‘ ë¹„êµ ì‹¤í—˜.

<br>

## ğŸ“š Part 1. Paper Review (PDF)
ì œê°€ ì§ì ‘ ì‘ì„±í•˜ê³  ì •ë¦¬í•œ BERT ë…¼ë¬¸ ë¶„ì„ ë°œí‘œ ìë£Œì…ë‹ˆë‹¤. ì•„ë˜ ë§í¬ë¥¼ í´ë¦­í•˜ë©´ ì „ì²´ ë‚´ìš©ì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> **[ğŸ“„ ë°œí‘œ ìë£Œ ë³´ëŸ¬ê°€ê¸° (Click to View PDF)](./BERT%20Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding_paper_study.pdf)**

### ğŸ’¡ Key Takeaways
* **Bidirectionality:** ê¸°ì¡´ ë‹¨ë°©í–¥(GPT)ì´ë‚˜ Shallow Bidirectional(ELMo) ëª¨ë¸ê³¼ ë‹¬ë¦¬, ëª¨ë“  ë ˆì´ì–´ì—ì„œ ì–‘ë°©í–¥ ë¬¸ë§¥ì„ ì°¸ì¡°í•˜ëŠ” **Deep Bidirectional** êµ¬ì¡°ë¥¼ ì œì•ˆ.
* **Pre-training Tasks:**
    * **Masked LM (MLM):** ì…ë ¥ì˜ 15%ë¥¼ ê°€ë¦¬ê³  ì˜ˆì¸¡í•˜ë©° ë¬¸ë§¥ì„ í•™ìŠµ.
    * **Next Sentence Prediction (NSP):** ë‘ ë¬¸ì¥ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ëŠ¥ë ¥ í•™ìŠµ.
* **Feature-based vs Fine-tuning:** BERTëŠ” Fine-tuningë¿ë§Œ ì•„ë‹ˆë¼, ì„ë² ë”©ë§Œ ì¶”ì¶œí•˜ì—¬ ì‚¬ìš©í•˜ëŠ” Feature-based ë°©ì‹ì—ì„œë„ SOTA ê¸‰ ì„±ëŠ¥ì„ ë³´ì„.

<br>
<hr>
<br>

## ğŸ“Š Part 2. Experimental Results (BERT vs GPT)
ì‹¤ì œ IMDb ì˜í™” ë¦¬ë·° ë°ì´í„°ì…‹(50k)ì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ë°©í–¥ ëª¨ë¸(OpenAI GPT)ê³¼ ì–‘ë°©í–¥ ëª¨ë¸(BERT)**ì˜ ì„±ëŠ¥ ì°¨ì´ë¥¼ ê²€ì¦í–ˆìŠµë‹ˆë‹¤.

### 1. Training Setup
* **Task:** Sentiment Analysis (Binary Classification)
* **Dataset:** IMDb Movie Reviews
* **Models:**
    * `OpenAI GPT` (110M params, Unidirectional)
    * `BERT-Base` (110M params, Bidirectional)
    * `BERT-Large` (340M params, Bidirectional)

### 2. Final Result Graph
**[ì‹¤í—˜ ê²°ê³¼ ìš”ì•½]**
í•™ìŠµ ì§„í–‰(Epoch)ì— ë”°ë¥¸ ì •í™•ë„(Accuracy) ë³€í™” ê·¸ë˜í”„ì…ë‹ˆë‹¤.

<p align="center">
  <img src="final_comparison.png" width="80%">
</p>

### 3. Quantitative Results (Accuracy)
| Model | Epoch 1 | Epoch 2 | Epoch 3 (Final) |
| :--- | :---: | :---: | :---: |
| **OpenAI GPT** | 87.9% | 87.9% | 89.1% |
| **BERT-Base** | 87.8% | 88.5% | 88.8% |
| **BERT-Large** | **88.8%** | **90.1%** | **89.9%** |

* **Observation:** BERT-Large ëª¨ë¸ì´ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë‚˜, 3 Epochì—ì„œ ê³¼ì í•©(Overfitting)ìœ¼ë¡œ ì¸í•´ ì„±ëŠ¥ì´ ì†Œí­ í•˜ë½í•¨.

### 4. Analysis
* **BERTì˜ ìŠ¹ë¦¬:** ë™ì¼í•œ íŒŒë¼ë¯¸í„° ìˆ˜(110M)ë¥¼ ê°€ì§„ `BERT-Base`ê°€ `OpenAI GPT`ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ê°ì„± ë¶„ì„ íƒœìŠ¤í¬ì—ì„œ **ì–‘ë°©í–¥ ë¬¸ë§¥ íŒŒì•…**ì´ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ ì¦ëª…í•©ë‹ˆë‹¤.
* **Size Matters:** ëª¨ë¸ í¬ê¸°ë¥¼ í‚¤ìš´ `BERT-Large`ëŠ” ì••ë„ì ì¸ ì„±ëŠ¥(ì•½ 89.9%)ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.
* **Overfitting ì´ìŠˆ:** `BERT-Large`ì˜ ê²½ìš° 3 Epochì—ì„œ ì„±ëŠ¥ì´ ì†Œí­ í•˜ë½í–ˆëŠ”ë°, ì´ëŠ” ëª¨ë¸ì´ ë„ˆë¬´ ê°•ë ¥í•˜ì—¬ í•™ìŠµ ë°ì´í„°ì— ê³¼ì í•©(Overfitting)ë˜ê¸° ì‹œì‘í–ˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. (ë…¼ë¬¸ ê¶Œì¥ Epoch: 2~4íšŒ)

<br>

## ğŸš€ How to Run
ë³¸ í”„ë¡œì íŠ¸ì˜ ì‹¤í—˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

### 1. Install Dependencies
í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤. `requirements.txt`ë¥¼ ì´ìš©í•˜ë©´ í•œ ë²ˆì— ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
pip install -r requirements.txt
