# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-1.8%2B-red)
![Task](https://img.shields.io/badge/Task-Paper__Review_%26_Implementation-green)

<br>

## ğŸ‘¨â€ğŸ’» Author
**Park Seung Hyun**
* **Affiliation:** Pusan National University (PNU), PNU CLINK Lab
* **Email:** shp09240000@pusan.ac.kr
* **GitHub:** [kook222](https://github.com/kook222)

<br>
<hr>
<br>

## ğŸ“Œ Project Overview
ì´ ì €ì¥ì†ŒëŠ” **BERT (Devlin et al., 2018)** ë…¼ë¬¸ì„ ê¹Šì´ ìˆê²Œ ë¶„ì„í•˜ê³ , ì‹¤ì œ ì½”ë“œ êµ¬í˜„ì„ í†µí•´ ê·¸ ì„±ëŠ¥ì„ ê²€ì¦í•œ ìŠ¤í„°ë”” ê¸°ë¡ì…ë‹ˆë‹¤.

í¬ê²Œ ë‘ ê°€ì§€ íŒŒíŠ¸ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤:
1.  **Paper Review:** ë…¼ë¬¸ì˜ í•µì‹¬ ì•„ì´ë””ì–´(MLM, NSP, Architecture) ì •ë¦¬ ë° ë°œí‘œ ìë£Œ ì œì‘.
2.  **Implementation:** IMDb ê°ì„± ë¶„ì„ íƒœìŠ¤í¬ë¥¼ í†µí•´ BERTì™€ GPTì˜ ì„±ëŠ¥ì„ ì§ì ‘ ë¹„êµ ì‹¤í—˜.

<br>

## ğŸ“š Part 1. Paper Review (PDF)
ì œê°€ ì§ì ‘ ì‘ì„±í•˜ê³  ì •ë¦¬í•œ BERT ë…¼ë¬¸ ë¶„ì„ ë°œí‘œ ìë£Œì…ë‹ˆë‹¤. ì•„ë˜ ë§í¬ë¥¼ í´ë¦­í•˜ë©´ ì „ì²´ ë‚´ìš©ì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> **[ğŸ“„ ë°œí‘œ ìë£Œ ë³´ëŸ¬ê°€ê¸° (Click to View PDF)](./BERT%20Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding_paper_study.pdf)**

### ğŸ’¡ Key Takeaways
* **Bidirectionality:** ê¸°ì¡´ ë‹¨ë°©í–¥(GPT)ì´ë‚˜ Shallow Bidirectional(ELMo) ëª¨ë¸ê³¼ ë‹¬ë¦¬, ëª¨ë“  ë ˆì´ì–´ì—ì„œ ì–‘ë°©í–¥ ë¬¸ë§¥ì„ ì°¸ì¡°í•˜ëŠ” **Deep Bidirectional** êµ¬ì¡°ë¥¼ ì œì•ˆ.
* **Pre-training Tasks:**
    * **Masked LM (MLM):** ì…ë ¥ì˜ 15%ë¥¼ ê°€ë¦¬ê³  ì˜ˆì¸¡í•˜ë©° ë¬¸ë§¥ì„ í•™ìŠµ.
    * **Next Sentence Prediction (NSP):** ë‘ ë¬¸ì¥ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ëŠ¥ë ¥ í•™ìŠµ.
* **Feature-based vs Fine-tuning:** BERTëŠ” Fine-tuningë¿ë§Œ ì•„ë‹ˆë¼, ì„ë² ë”©ë§Œ ì¶”ì¶œí•˜ì—¬ ì‚¬ìš©í•˜ëŠ” Feature-based ë°©ì‹ì—ì„œë„ SOTA ê¸‰ ì„±ëŠ¥ì„ ë³´ì„.

<br>

## ğŸ“Š Part 2. Experimental Results (BERT vs GPT)
ì‹¤ì œ IMDb ì˜í™” ë¦¬ë·° ë°ì´í„°ì…‹(50k)ì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ë°©í–¥ ëª¨ë¸(OpenAI GPT)ê³¼ ì–‘ë°©í–¥ ëª¨ë¸(BERT)**ì˜ ì„±ëŠ¥ ì°¨ì´ë¥¼ ê²€ì¦í–ˆìŠµë‹ˆë‹¤.

### 1. Training Setup
* **Task:** Sentiment Analysis (Binary Classification)
* **Dataset:** IMDb Movie Reviews
* **Models:**
    * `OpenAI GPT` (110M params, Unidirectional)
    * `BERT-Base` (110M params, Bidirectional)
    * `BERT-Large` (340M params, Bidirectional)

### 2. Final Result Graph
**[ì‹¤í—˜ ê²°ê³¼ ìš”ì•½]**
í•™ìŠµ ì§„í–‰(Epoch)ì— ë”°ë¥¸ ì •í™•ë„(Accuracy) ë³€í™” ê·¸ë˜í”„ì…ë‹ˆë‹¤.

<p align="center">
  <img src="final_comparison.png" width="80%">
</p>

### 3. Quantitative Results (Accuracy)
| Model | Epoch 1 | Epoch 2 | Epoch 3 (Final) |
| :--- | :---: | :---: | :---: |
| **OpenAI GPT** | 87.9% | 87.9% | 89.1% |
| **BERT-Base** | 87.8% | 88.5% | 88.8% |
| **BERT-Large** | **88.8%** | **90.1%** | **89.9%** |

* **Observation:** BERT-Large ëª¨ë¸ì´ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë‚˜, 3 Epochì—ì„œ ê³¼ì í•©(Overfitting)ìœ¼ë¡œ ì¸í•´ ì„±ëŠ¥ì´ ì†Œí­ í•˜ë½í•¨.

### 4. Analysis
* **BERTì˜ ìŠ¹ë¦¬:** ë™ì¼í•œ íŒŒë¼ë¯¸í„° ìˆ˜(110M)ë¥¼ ê°€ì§„ `BERT-Base`ê°€ `OpenAI GPT`ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ê°ì„± ë¶„ì„ íƒœìŠ¤í¬ì—ì„œ **ì–‘ë°©í–¥ ë¬¸ë§¥ íŒŒì•…**ì´ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ ì¦ëª…í•©ë‹ˆë‹¤.
* **Size Matters:** ëª¨ë¸ í¬ê¸°ë¥¼ í‚¤ìš´ `BERT-Large`ëŠ” ì••ë„ì ì¸ ì„±ëŠ¥(ì•½ 89.9%)ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.
* **Overfitting ì´ìŠˆ:** `BERT-Large`ì˜ ê²½ìš° 3 Epochì—ì„œ ì„±ëŠ¥ì´ ì†Œí­ í•˜ë½í–ˆëŠ”ë°, ì´ëŠ” ëª¨ë¸ì´ ë„ˆë¬´ ê°•ë ¥í•˜ì—¬ í•™ìŠµ ë°ì´í„°ì— ê³¼ì í•©(Overfitting)ë˜ê¸° ì‹œì‘í–ˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. (ë…¼ë¬¸ ê¶Œì¥ Epoch: 2~4íšŒ)

<br>

## ğŸš€ How to Run
ë³¸ í”„ë¡œì íŠ¸ëŠ” í•™ìŠµ ë‹¨ê³„ë³„, ëª¨ë¸ë³„ë¡œ ì½”ë“œê°€ ë¶„ë¦¬ë˜ì–´ ìˆì–´ ì§ê´€ì ì¸ ì‹¤í–‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

### 1. Install Dependencies
í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.

```bash
pip install -r requirements.txt

```

### 2. Training (Run Experiments)

ì›í•˜ëŠ” ëª¨ë¸ì˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì§ì ‘ ì‹¤í–‰í•˜ì—¬ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.

**A. Train BERT-Base**

```bash
# BERT ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ
python 1_train_bert.py

```

**B. Train BERT-Large**

```bash
# BERT Large ëª¨ë¸ í•™ìŠµ
python 1_train_bert_large.py

```

**C. Train OpenAI-GPT**

```bash
# GPT ëª¨ë¸ í•™ìŠµ (ë¹„êµêµ°)
python 1_train_gpt.py

```

### 3. Inference & Testing

í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ ê²°ê³¼ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

```bash
# BERT Base ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
python 3_predict_bert.py

# BERT Large ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
python 3_predict_bert_large.py

# GPT ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
python 3_predict_gpt.py

```

### 4. Visualization

í•™ìŠµ ë¡œê·¸(`training_history.json`)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ê²°ê³¼ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```bash
python 4_final_graph.py

```

## ğŸ“‚ Project Structure

ì´ í”„ë¡œì íŠ¸ì˜ ë””ë ‰í† ë¦¬ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```bash
.
â”œâ”€â”€ 1_train_bert.py        # BERT Base Training script
â”œâ”€â”€ 1_train_bert_large.py  # BERT Large Training script
â”œâ”€â”€ 1_train_gpt.py         # GPT Training script
â”œâ”€â”€ 3_predict_bert.py      # BERT Base Inference script
â”œâ”€â”€ 3_predict_bert_large.py # BERT Large Inference script
â”œâ”€â”€ 3_predict_gpt.py       # GPT Inference script
â”œâ”€â”€ 4_final_graph.py       # Result Visualization
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ README.md              # Project documentation
â””â”€â”€ ...

```

## ğŸ›  Tech Stack

* **Language:** Python 3.8+
* **Framework:** PyTorch, Hugging Face Transformers
* **Visualization:** Matplotlib, Seaborn

## ğŸ”— References

* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
* [Improving Language Understanding by Generative Pre-Training (GPT)](https://www.google.com/search?q=https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
* [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers/index)
