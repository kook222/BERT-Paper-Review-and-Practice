# -*- coding: utf-8 -*-
import torch
import numpy as np
import pandas as pd
import os
import json
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset
import evaluate

# 1. ì„¤ì •
device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
print(f"ğŸ”¥ í•™ìŠµ ì¥ì¹˜: {device} (ëª¨ë¸: BERT-Large - ëíŒì™•)")

# 2. ë°ì´í„° ì¤€ë¹„ (ê¸°ì¡´ íŒŒì¼ ì¬ì‚¬ìš©)
print("ğŸ“¥ ë°ì´í„° ë¡œë”© ì¤‘...")
if not os.path.exists("train.parquet"):
    print("   -> (í˜¹ì‹œ ëª°ë¼ ë‹¤ìš´ë¡œë“œ ì½”ë“œ ìœ ì§€) í•™ìŠµ ë°ì´í„° ë°›ëŠ” ì¤‘...")
    df_train = pd.read_parquet("https://huggingface.co/datasets/imdb/resolve/main/plain_text/train-00000-of-00001.parquet")
    df_train.to_parquet("train.parquet")
    df_test = pd.read_parquet("https://huggingface.co/datasets/imdb/resolve/main/plain_text/test-00000-of-00001.parquet")
    df_test.to_parquet("test.parquet")

data_files = {"train": "train.parquet", "test": "test.parquet"}
dataset = load_dataset("parquet", data_files=data_files)

train_dataset = dataset["train"].shuffle(seed=42)
eval_dataset = dataset["test"].shuffle(seed=42).select(range(1000))

# 3. ëª¨ë¸ ì„¤ì • (BERT-Large)
model_name = "bert-large-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

print("âš™ï¸ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
tokenized_train = train_dataset.map(tokenize_function, batched=True)
tokenized_eval = eval_dataset.map(tokenize_function, batched=True)

model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)
model.to(device)

# 4. í‰ê°€ í•¨ìˆ˜
metric = evaluate.load("accuracy")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# 5. í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./bert_large_result",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    # [ì¤‘ìš”] ëª¨ë¸ì´ ì»¤ì„œ ë©”ëª¨ë¦¬ í„°ì§ˆê¹Œë´ 16 -> 8ë¡œ ì¤„ì˜€ìŠµë‹ˆë‹¤.
    # ë§Œì•½ ê·¸ë˜ë„ "Out of Memory" ì—ëŸ¬ê°€ ë‚˜ë©´ 4ë¡œ ë” ì¤„ì´ì„¸ìš”.
    per_device_train_batch_size=8, 
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir='./logs_bert_large',
    dataloader_pin_memory=False 
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    compute_metrics=compute_metrics,
)

# 6. í•™ìŠµ ì‹¤í–‰
print("\nğŸš€ [BERT-Large] í•™ìŠµ ì‹œì‘! (ì‹œê°„ì´ Baseë³´ë‹¤ 2~3ë°° ë” ê±¸ë¦½ë‹ˆë‹¤)")
trainer.train()

# 7. ê¸°ë¡ ì €ì¥
history = []
for log in trainer.state.log_history:
    if 'eval_accuracy' in log:
        history.append({'epoch': log['epoch'], 'accuracy': log['eval_accuracy']})

with open('training_history_bert_large.json', 'w') as f:
    json.dump(history, f)

print("\nâœ… BERT-Large í•™ìŠµ ì™„ë£Œ!")