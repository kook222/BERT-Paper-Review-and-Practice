# -*- coding: utf-8 -*-
# íŒŒì¼ëª…: 3_predict_gpt.py
import torch
from transformers import OpenAIGPTTokenizer, OpenAIGPTForSequenceClassification

# 1. ì„¤ì • (í•™ìŠµëœ GPT ëª¨ë¸ í´ë” ê²½ë¡œ)
model_path = "./gpt_result"  # GPT í•™ìŠµì´ ëë‚˜ë©´ ì´ í´ë”ê°€ ìƒê¹ë‹ˆë‹¤.
device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")

print(f"ğŸ”¥ ì‚¬ìš© ì¥ì¹˜: {device}")
print("ğŸ“‚ í•™ìŠµëœ GPT ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")

try:
    # 2. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
    # [ì£¼ì˜] GPTëŠ” íŒ¨ë”© í† í°ì´ ì—†ì–´ì„œ í•™ìŠµ ë•Œì²˜ëŸ¼ unk_tokenìœ¼ë¡œ ì„¤ì •í•´ì¤˜ì•¼ í•¨
    tokenizer = OpenAIGPTTokenizer.from_pretrained("openai-gpt")
    tokenizer.pad_token = tokenizer.unk_token
    
    # ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸(checkpoint) í´ë”ê°€ ì•„ë‹ˆë¼, trainer.save_model()ë¡œ ì €ì¥í•œ ìµœì¢… ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì•¼ í•¨
    # ë§Œì•½ ì—ëŸ¬ê°€ ë‚˜ë©´ model_pathë¥¼ "./gpt_result/checkpoint-xxxx" í˜•íƒœë¡œ ë°”ê¿”ë³´ì„¸ìš”.
    model = OpenAIGPTForSequenceClassification.from_pretrained(model_path)
    model.to(device)
    model.eval() # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜

except Exception as e:
    print(f"âŒ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµì´ ë¨¼ì € ì™„ë£Œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\nì—ëŸ¬: {e}")
    exit()

print("âœ… GPT ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ì¢…ë£Œí•˜ë ¤ë©´ 'q' ì…ë ¥)")

# 3. ì˜ˆì¸¡ ë£¨í”„
labels = {0: "ğŸ‘ ë¶€ì • (Negative)", 1: "ğŸ‘ ê¸ì • (Positive)"}

while True:
    text = input("\nğŸ“ ì˜í™” ë¦¬ë·°ë¥¼ ì˜ì–´ë¡œ ì…ë ¥í•˜ì„¸ìš”: ")
    if text.lower() == 'q':
        break
    
    # ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬
    inputs = tokenizer(text, return_tensors="pt", padding="max_length", truncation=True, max_length=128)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # ì˜ˆì¸¡
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = torch.nn.functional.softmax(logits, dim=-1)
        pred = torch.argmax(probs, dim=-1).item()
        confidence = probs[0][pred].item() * 100

    print(f"   ğŸ¤– GPTì˜ íŒë‹¨: {labels[pred]} (í™•ì‹ : {confidence:.2f}%)")